{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Python for Spatial Analysis\n## Second part of the module of GG3209 Spatial Analysis with GIS.\n### Notebook to learn and practice Pandas\n\n---\nDr Fernando Benitez -  University of St Andrews - School of Geography and Sustainable Development - Third Iteration 2024.1"},{"metadata":{},"cell_type":"markdown","source":"### Introduction \n\nAfter practicing **NumPy**, this notebook aims to work with the library **Pandas** which allows for reading in and working with data tables.\n\nMost geo spatial scientists are first introduced to data tables in the form of an Excel Spreadsheet. In such a structure, each record or feature is represented by a row of data while each column represents a specific piece of information for each record. Sometimes we call that a variable, or attribute. \n\nFurther, spreadsheets are able to hold different data types in each column. A comparable data structure would be handy for use in Python. This is made available by the **Pandas** library. Pandas allows for data to be stored in **DataFrames**. If you work in the R environment, this is very similar to the concept of data frames in R. In fact, *Pandas DataFrames were inspired by R data frames*. \n\nThe complete documentation for Pandas can be found [here](https://pandas.pydata.org/).\n\nAfter working through this module you will be able to:\n\n1. Create and manipulate **Series** and **DataFrames** using Pandas.\n2. Query and subset DataFrames.\n3. Manipulate DataFrames.\n4. Summarize and group DataFrames."},{"metadata":{},"cell_type":"markdown","source":"## Pandas"},{"metadata":{},"cell_type":"markdown","source":"### Intro to Series"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we talk about DataFrames, I will introduce the concept of a **Series**. These are actually very similar to a NumPy array except that they allow for axis labels to be assigned. Examples of generating a series from **lists**, **NumPy arrays**, and **dictionaries** are provided below. \n\nA series is comparable to a single column from a Spreadsheet."},{"metadata":{"trusted":false},"cell_type":"code","source":"lst1 = [\"GIS\", \"Remote Sensing\", \"Spatial Analysis\", \"Digital Cartography\"]\narr1 = np.array([350, 455, 457, 642])\ndict1 = {'Class1':\"GIS\", \"Class2\":\"Remote Sensing\", \"Class3\":\"Spatial Analysis\", \"Class4\":\"Digital Cartography\"}\n\ns_lst = pd.Series(data=lst1, index = [\"Class1\", \"Class2\", \"Class3\", \"Class4\"])\ns_arr = pd.Series(data=arr1, index = [\"Class1\", \"Class2\", \"Class3\", \"Class4\"])\ns_dict = pd.Series(dict1)\n\nprint(s_lst)\nprint(s_arr)\nprint(s_dict)","execution_count":3,"outputs":[{"output_type":"stream","text":"Class1                    GIS\nClass2         Remote Sensing\nClass3       Spatial Analysis\nClass4    Digital Cartography\ndtype: object\nClass1    350\nClass2    455\nClass3    457\nClass4    642\ndtype: int64\nClass1                    GIS\nClass2         Remote Sensing\nClass3       Spatial Analysis\nClass4    Digital Cartography\ndtype: object\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Labels or names can then be used to select data either using **bracket notation** or **dot notation**. \n\nYou can use whichever method you prefer. However, if you use dot notation you should not included spaces in the column names. "},{"metadata":{"trusted":false},"cell_type":"code","source":"print(s_dict[\"Class3\"])\nprint(s_dict.Class3)","execution_count":4,"outputs":[{"output_type":"stream","text":"Spatial Analysis\nSpatial Analysis\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Intro to DataFrames"},{"metadata":{},"cell_type":"markdown","source":"Let's start by building a DataFrame from a set of lists. First, you can create three lists to hold different components of course title information.\n\nNext, you could combine these lists into a dictionary. Finally, you can convert the dictionary into a DataFrame. \n\nNote that a well formatted table is generated by just calling the DataFrame name without the *print()* function; however, in the example we use *print()* (but try out both ways to see the difference). Also, the **keys** from the dictionary have been used as the column names, and a default index has been assigned to each row.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"prefix = [\"Geol\", \"Geol\", \"Geol\", \"Geol\", \"Geog\", \"Geog\", \"Geog\"]\ncnum = [103, 321, 331, 341, 350, 455, 462]\ncname = [\"Earth Through Time\", \"Geomorphology\", \"Paleontology\", \"Structural Geology\", \"GIScience\", \"Remote Sensing\", \"Digital Cartography\"]\ncourse_dict = {\"prefix\": prefix, \"course_number\": cnum, \"course_name\": cname}\ncourse_df = pd.DataFrame(course_dict)\n#course_df\nprint(course_df)","execution_count":4,"outputs":[{"output_type":"stream","text":"  prefix  course_number          course_name\n0   Geol            103   Earth Through Time\n1   Geol            321        Geomorphology\n2   Geol            331         Paleontology\n3   Geol            341   Structural Geology\n4   Geog            350            GIScience\n5   Geog            455       Remote Sensing\n6   Geog            462  Digital Cartography\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Since column names are assigned, they can be used to select out individual columns using bracket or dot notation. Single columns can be saved as a Series. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(course_df[\"course_name\"])\n# Or course_df.course_name","execution_count":5,"outputs":[{"output_type":"stream","text":"0     Earth Through Time\n1          Geomorphology\n2           Paleontology\n3     Structural Geology\n4              GIScience\n5         Remote Sensing\n6    Digital Cartography\nName: course_name, dtype: object\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"A list of column names can be provided to subset out multiple columns. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(course_df[[\"course_number\", \"course_name\"]])","execution_count":6,"outputs":[{"output_type":"stream","text":"   course_number          course_name\n0            103   Earth Through Time\n1            321        Geomorphology\n2            331         Paleontology\n3            341   Structural Geology\n4            350            GIScience\n5            455       Remote Sensing\n6            462  Digital Cartography\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Indexing with ```.loc``` and ```.iloc```\n\nPandas created the methods **.loc[]** and **.iloc[]** as more flexible alternatives for accessing data from a dataframe. \n\nUse ```df.iloc[]``` for indexing with integers and ```df.loc[]``` for indexing with labels.\n\nThese are typically the recommended methods of indexing in Pandas\n\n1. Using the **.loc** method, you can subset based on **column names and row labels** combined, and \n\n2. The **.iloc** method, in contrast, is used for **selection based on indexes**."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(course_df.loc[[1, 2, 4],[\"course_number\", \"course_name\"]])\n# Or course_df.iloc[[1, 2, 4],[1, 2]]","execution_count":7,"outputs":[{"output_type":"stream","text":"   course_number    course_name\n1            321  Geomorphology\n2            331   Paleontology\n4            350      GIScience\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"You can even use the data stored in existing columns to create a new column. Note that the new column does not need to be declared prior to writing to it. In the example, I have written the entire course name to a new column. The *map()* method is used to make sure all data are treated as strings. It allow for the same function, in this case **str()**, to be applied to each element in an iterable, in this case each row in the DataFrame. I am including blank spaces so that the components are not ran together. "},{"metadata":{"trusted":false},"cell_type":"code","source":"course_df[\"full_name\"] = course_df[\"prefix\"].map(str)  + \" \" + course_df[\"course_number\"].map(str)  + \": \" + course_df[\"course_name\"].map(str) \nprint(course_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading external files\n\nInstead of creating data tables or DataFrames manually, you are probably more likely to read in a data table from a file (CSV) or web link. \n\nFortunately, **Pandas** provides functions for reading data in from a variety of formats. Here are some examples:\n\n* *read_table()*: delimited file (TXT, CSV, etc.)\n* *read_csv()*: comma-separated values (CSV)\n* *read_excel()*: Excel Spreadsheet\n* *read_json()*: JavaScript Object Notation (JSON)\n* *read_html()*: HTML table\n* *read_sas()*: SAS file\n\nFull documentation on reading in data can be found [here](https://pandas.pydata.org/docs/reference/io.html).\n\nIn the example below, I am reading in a CSV file from my local computer. The *sep* argument is used to define the deliminator ( like you have done in Excel).\n\nHowever, commas are the default, so *it isn't necessary to include this argument in this case*.\n\nSetting the *header* argument to 0 indicated that the first row of data should be treated as column names or headers. \n\nIt isn't always necessary to specify the character encoding; But a best practice tells that it is necessary due to the use of special characters in some tables. \n\nTo view the first 10 rows of the data. You can use the *head()* method.\n\nThe *len()* function returns the number of rows. "},{"metadata":{"trusted":false},"cell_type":"code","source":"cities_df = pd.read_csv('/arcgis/home/world_cities.csv', sep=\",\", header=0, encoding=\"ISO-8859-1\")\ncities_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(len(cities_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DataFrame Query and Subset"},{"metadata":{},"cell_type":"markdown","source":"Let's now use this data table to explore data query and selection methods. \n\nIn the first example, I am selecting out Italian cities and saving them to a new DataFrame. Note the use of bracket notation. The code in the middle bracket is used to perform the selection. \n\nThe second example includes a compound query. Note the use of parenthesis within the query. \n\nLastly, it is also possible to subset out only certain columns that meet the query. In the last example, I am subsetting out just the name of the city and population. "},{"metadata":{"trusted":false},"cell_type":"code","source":"#Example 1\njust_italy = cities_df[cities_df[\"country\"]==\"Italy\"]\nprint(just_italy.head(10))\nprint('')\nprint(len(just_italy))\nprint('')\n#Example 2\nitaly_med_cities = cities_df[(cities_df[\"country\"]==\"Italy\") & (cities_df[\"pop\"]>500000)]\nprint(italy_med_cities)\nprint('')\nprint(len(italy_med_cities))\nprint('')\n#Example 3\nitaly_cities_pop = cities_df[(cities_df[\"country\"]==\"Italy\") & (cities_df[\"pop\"]>500000)][[\"city\", \"pop\"]]\nprint(italy_cities_pop.head(3))\nprint('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another option for performing queries is to use the **query()** method provided by Pandas. When using this method, the query will need to be provided as an expression in string form. Also, spaces in column names can be problematic, so spaces should be removed or replaced with underscores. "},{"metadata":{"trusted":false},"cell_type":"code","source":"#Remove spaces in column names using list comprehension\ncities_df.columns = [column.replace(\" \", \"_\") for column in cities_df.columns]\n#Example 1\njust_spain = cities_df.query('country==\"Spain\"')\nprint(just_spain.head(10))\nprint('')\nprint(len(just_spain))\nprint('')\n#Example 2\nspanish_cities_df = cities_df.query('country==\"Spain\" and pop > 500000')\nprint(len(spanish_cities_df))\nprint('')\n#Example 3\nsubset_query = cities_df.query('country==\"Spain\" and pop > 500000')[[\"city\", \"pop\"]]\nprint(subset_query)\nprint('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once a query is complete, you may want to save the result back to a file on your local machine. The code below provides an example for saving out the last subset of data to a CSV file. The Pandas documentation provides examples for saving to other formats. "},{"metadata":{"trusted":false},"cell_type":"code","source":"subset_query.to_csv('/arcgis/home/subset_data.csv', sep=\",\", header=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing Values"},{"metadata":{},"cell_type":"markdown","source":"The *NULL*, *NoData*, or missing indicator in Python is *NaN*. To begin exploring missing values, let's recode some of the data to *NaN* in the cities data set. In the example below, I am changing the \"Germany\" and \"Palestine\" countries to *NaN*. I am also recoding any population between 10.000 and 50.000 to *NaN*. The *replace()* method is used to change the categories while the *mask()* method is used to recode the rating values. *np.nan* is a NumPy method for defining null values.  \n\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"cities_nan = cities_df.copy()\ncities_nan[\"country\"] = cities_nan[[\"country\"]].replace([\"Germany\", \"Palestine\"], np.nan)\ncities_nan['pop'].mask(cities_nan['pop'].between(10000, 50000), inplace=True)\nprint(cities_nan.head(10))\nprint('')\nprint(len(cities_nan))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The *dropna()* method can be used to remove rows or columns that contain missing data. If the axis parameter is set to 0, rows with missing values in any column will be removed. If it is set to 1, columns with missing data in any row will be removed. "},{"metadata":{"trusted":false},"cell_type":"code","source":"cities_drop = cities_nan.dropna(axis=0)\nprint(cities_drop.head(10))\nprint('')\nprint(len(cities_drop))\nprint('')\ncities_dropc = cities_nan.dropna(axis=1)\nprint(cities_dropc.head(10))\nprint('')\nprint(len(cities_dropc))\nprint('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The *.fillna()* method can be used to replace NA values with another value or string. In the example, I am changing the missing genres to \"Unknown Country\"."},{"metadata":{"trusted":false},"cell_type":"code","source":"cities_nan[\"country\"] = cities_nan[\"country\"].fillna(value=\"Unknown Country\")\nprint(cities_nan.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is also possible to replace null values with a statistic derived from the available values. In the example, I am replacing the missing population values with the mean of all available in the attribute. Of course this is not possible, here is just an example."},{"metadata":{"trusted":false},"cell_type":"code","source":"cities_nan[\"pop\"] = cities_nan[\"pop\"].fillna(value=cities_nan[\"pop\"].mean())\nprint(cities_nan.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grouping and Summarizing DataFrames"},{"metadata":{},"cell_type":"markdown","source":"Pandas provides methods for summarizing data as described in the examples below. First, I am creating individual statistics and saving them to variables. I then create a Series from a dictionary of these statistics, convert it to a DataFrame using the *to_frame()* method, then transpose the DataFrame using *transpose()*."},{"metadata":{"trusted":false},"cell_type":"code","source":"earthquake_df = pd.read_csv('/arcgis/home/Latest_earthquake_world.csv', sep=\",\", header=0, encoding=\"ISO-8859-1\")\nearthquake_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"earthquake_df = pd.read_csv('/arcgis/home/Latest_earthquake_world.csv', sep=\",\", header=0, encoding=\"ISO-8859-1\")\nearthquake_df.columns = [column.replace(\" \", \"_\") for column in earthquake_df.columns]\nearth_cnt = earthquake_df[\"mag\"].count()\nearth_mn = earthquake_df[\"mag\"].mean()\nearth_max = earthquake_df[\"mag\"].max()\nearth_min = earthquake_df[\"mag\"].min()\nearth_rang = earth_max-earth_min\nearth_stats= pd.Series({\"Count\": earth_cnt, \"Mean\": earth_mn, \"Max\": earth_max, \"Min\": earth_min, \"Range\": earth_rang}).to_frame().transpose()\nprint(earth_stats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is also possible to obtain summary statistics for each group separately by applying the very useful *group_by()* method. In the example below, I am obtaining stats for each MagSource and saving them into a DataFrame. The columns do not need to be defined beforehand. "},{"metadata":{"trusted":false},"cell_type":"code","source":"earthquake_df = pd.read_csv('/arcgis/home/Latest_earthquake_world.csv', sep=\",\", header=0, encoding=\"ISO-8859-1\")\nearthquake_df.columns = [column.replace(\" \", \"_\") for column in earthquake_df.columns]\nearthquake_stats = pd.DataFrame()\nearthquake_stats[\"Count\"] = earthquake_df.groupby(\"magSource\")['mag'].count()\nearthquake_stats[\"Mean\"] = earthquake_df.groupby(\"magSource\")['mag'].mean()\nearthquake_stats[\"Max\"] = earthquake_df.groupby(\"magSource\")['mag'].max()\nearthquake_stats[\"Min\"] = earthquake_df.groupby(\"magSource\")['mag'].min()\nearthquake_stats[\"Range\"] = earthquake_stats[\"Max\"] - earthquake_stats[\"Min\"]\nearthquake_stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The *describe()* method can be used to obtain a set of default summary statistics for a column of data. Combining this with *group_by()* allows for the calculation of statistics by group. "},{"metadata":{"trusted":false},"cell_type":"code","source":"print(earthquake_df.groupby(\"mag\")[\"depth\"].describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Concatenate and Merge"},{"metadata":{},"cell_type":"markdown","source":"The Pandas *concat()* method is used to **concatenate** DataFrames that have the same columns. This is comparable to copying and pasting rows from two spreadsheets into a new spreadsheet. To demonstrate this, I have extracted rows using indexes. Next, I concatenate them back to a new DataFrame. "},{"metadata":{"trusted":false},"cell_type":"code","source":"earthquake_df = pd.read_csv('/arcgis/home/Latest_earthquake_world.csv', sep=\",\", header=0, encoding=\"ISO-8859-1\")\nearthquake_df.columns =[column.replace(\" \", \"_\") for column in earthquake_df.columns]\n\nearth_sub1 = earthquake_df[10:100]\nearth_sub2 = earthquake_df[200:300]\nearth_subc = pd.concat([earth_sub1, earth_sub2])\nprint(len(earthquake_df))\nprint(len(earth_subc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Merge** is comparable to table joins when using SQL. This requires the use of **keys** and the declaration of a joining method, such as \"Left\", \"Right\", \"Inner\", or \"Outer\". \n\nIn the example, I first create a unique ID by copying the row index to a column (idx).\n\nI then break the data into two components, each containing the ID and a subset of the remaining columns. \n\nI then use the *merge()* method to merge the DataFrames using the \"inner\" method and the common \"id\" field. \"Inner\" will only return rows that occur in both data sets. Since both DataFrames were derived from the same original DataFrame, they will have identical rows, so the result would be the same as using \"left\", where all rows from the left table are maintained even if they don't occur in the right table, or \"right\", where all rows from the right table are maintained even if they don't occur in the left table. \n\nIn the second example, I use a query to separate out only earthquakes with magnitude of more than 4.0. When I perform a join with all of the data using the \"inner\" method, I only get back the common or shared rows. \n\nNote that there is also a *join()* method that joins based on indexes. However, that will not be demonstrated here. "},{"metadata":{"trusted":false},"cell_type":"code","source":"earthquake_df = pd.read_csv('/arcgis/home/Latest_earthquake_world.csv', sep=\",\", header=0, encoding=\"ISO-8859-1\")\n\nearthquake_df[\"idx\"] = earthquake_df.index\nprint(earthquake_df.head(2))\nprint('')\nprint(len(earthquake_df))\nprint('')\nsubset_first = earthquake_df[[\"idx\", \"mag\"]]\nsubset_second = earthquake_df[[\"idx\", \"depth\", \"place\"]]\n\nearth_merge = pd.merge(subset_first, subset_second, how=\"inner\", on=\"idx\")\nprint(earth_merge.head(5))\nprint('')\nprint(len(earth_merge))\nprint('')\nsubset_third = earthquake_df.query('mag > 4')[[\"idx\", \"place\", \"depth\"]]\nearth_merge2 = pd.merge(subset_first, subset_third, how=\"inner\", on=\"idx\")\nprint(earth_merge2.head(5))\nprint('')\nprint(len(earth_merge2))\nprint('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final remarks"},{"metadata":{},"cell_type":"markdown","source":"Well there is much to discuss and learn from the use of Pandas, but this is the initial step to get you familiarize with this library. Now it is your turn to try out the exercises to help you to recall and apply all the methods in these two notebook. So please open the **Exercises_NumPy_Pandas.ipynb** work on it. \n\nFor more examples and details, please consult the documentation for [Pandas](https://pandas.pydata.org/docs/reference/io.html). \n\nIn the next week, we will discuss methods for graphing and visualizing data using **matplotlib**, and **GeoPandas**. "}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.9.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"a62a218f45948969006c944db2ca1c519af623da5e08f864ae6aafcacb945df1"}}},"nbformat":4,"nbformat_minor":4}